"""
å„ªåŒ–å¾Œçš„éåŒæ­¥è¨“ç·´ä»»å‹™
ä¿®æ­£äº†è¶…åƒæ•¸å„ªåŒ–ã€pkg_resources è­¦å‘Šã€å¤šé€²ç¨‹å•é¡Œç­‰
"""

import pandas as pd
import numpy as np
import os
import uuid
import pickle
import warnings
from datetime import datetime
import logging
from typing import Dict, Any
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import StackingClassifier
from sklearn.model_selection import cross_val_score, StratifiedKFold, train_test_split
from sklearn.metrics import roc_auc_score
from tqdm import tqdm
import sys
from celery import Celery

# å‰µå»º Celery æ‡‰ç”¨ç¨‹å¼å¯¦ä¾‹
celery = Celery(
    'loan_approval_tasks',
    broker='redis://redis:6379/0',
    backend='redis://redis:6379/0'
)

# è§£æ±º pkg_resources è­¦å‘Š
warnings.filterwarnings("ignore", category=DeprecationWarning, module="pkg_resources")
warnings.filterwarnings("ignore", category=UserWarning, module="multiprocessing")

# HyperOpt ç›¸é—œ
try:
    from hyperopt import fmin, tpe, hp, STATUS_OK, Trials
    HYPEROPT_AVAILABLE = True
except ImportError:
    HYPEROPT_AVAILABLE = False
    print("HyperOpt æœªå®‰è£ï¼Œå°‡è·³éè¶…åƒæ•¸å„ªåŒ–")

# æ¨¡å‹ç›¸é—œ
try:
    import lightgbm as lgb
    LIGHTGBM_AVAILABLE = True
except ImportError:
    LIGHTGBM_AVAILABLE = False
    print("LightGBM æœªå®‰è£")

try:
    import xgboost as xgb
    XGBOOST_AVAILABLE = True
except ImportError:
    XGBOOST_AVAILABLE = False
    print("XGBoost æœªå®‰è£")

from .model import StackingModel
from .preprocessing import AdvancedDataPreprocessor
from .utils import save_model, generate_model_id

logger = logging.getLogger(__name__)

@celery.task
def optimize_hyperparameters_task(
    job_id: str,
    X: pd.DataFrame,
    y: pd.Series,
    n_trials: int = 50,
    training_jobs: Dict[str, Any] = None
):
    """
    å„ªåŒ–å¾Œçš„è¶…åƒæ•¸å„ªåŒ–å‡½æ•¸
    ä¿®æ­£äº†æœç´¢ç©ºé–“ã€ç›®æ¨™å‡½æ•¸å’Œå¤šé€²ç¨‹å•é¡Œ
    
    Args:
        job_id: ä»»å‹™ ID
        X: ç‰¹å¾µæ•¸æ“š
        y: ç›®æ¨™è®Šæ•¸
        n_trials: å„ªåŒ–è©¦é©—æ¬¡æ•¸
        training_jobs: ä»»å‹™ç‹€æ…‹å­—å…¸
    """
    try:
        if not HYPEROPT_AVAILABLE:
            logger.error("HyperOpt æœªå®‰è£ï¼Œç„¡æ³•é€²è¡Œè¶…åƒæ•¸å„ªåŒ–")
            if training_jobs:
                training_jobs[job_id]["status"] = "FAILURE"
                training_jobs[job_id]["message"] = "HyperOpt æœªå®‰è£"
            return
        
        # æª¢æŸ¥æ¨¡å‹å¯ç”¨æ€§
        if not (LIGHTGBM_AVAILABLE and XGBOOST_AVAILABLE):
            logger.error("LightGBM æˆ– XGBoost æœªå®‰è£")
            if training_jobs:
                training_jobs[job_id]["status"] = "FAILURE"
                training_jobs[job_id]["message"] = "æ¨¡å‹åº«æœªå®‰è£"
            return
        
        logger.info(f"é–‹å§‹ä½¿ç”¨ HyperOpt é€²è¡Œè¶…åƒæ•¸å„ªåŒ–ä»»å‹™: {job_id}")
        
        if training_jobs:
            training_jobs[job_id]["status"] = "PROCESSING"
            training_jobs[job_id]["message"] = "æ­£åœ¨ä½¿ç”¨ HyperOpt å„ªåŒ–è¶…åƒæ•¸..."
        
        # å„ªåŒ–å¾Œçš„æœç´¢ç©ºé–“ - æ“´å¤§åƒæ•¸ç¯„åœä¸¦å¢åŠ å¤šæ¨£æ€§
        space = {
            # LightGBM è¶…åƒæ•¸ - ä½¿ç”¨é›¢æ•£é¸æ“‡è€Œéé€£çºŒç¯„åœ
            'lgbm_n_estimators': hp.choice('lgbm_n_estimators', [100, 200, 300, 500, 800, 1000]),
            'lgbm_learning_rate': hp.loguniform('lgbm_learning_rate', np.log(0.01), np.log(0.3)),
            'lgbm_num_leaves': hp.choice('lgbm_num_leaves', [31, 63, 127, 255, 511]),
            'lgbm_max_depth': hp.choice('lgbm_max_depth', [-1, 3, 5, 7, 9, 12, 15]),
            'lgbm_min_child_samples': hp.choice('lgbm_min_child_samples', [10, 20, 50, 100]),
            'lgbm_subsample': hp.uniform('lgbm_subsample', 0.6, 1.0),
            'lgbm_colsample_bytree': hp.uniform('lgbm_colsample_bytree', 0.6, 1.0),
            'lgbm_reg_alpha': hp.loguniform('lgbm_reg_alpha', np.log(0.01), np.log(10)),
            'lgbm_reg_lambda': hp.loguniform('lgbm_reg_lambda', np.log(0.01), np.log(10)),
            
            # XGBoost è¶…åƒæ•¸ - ä½¿ç”¨é›¢æ•£é¸æ“‡
            'xgb_n_estimators': hp.choice('xgb_n_estimators', [100, 200, 300, 500, 800, 1000]),
            'xgb_learning_rate': hp.loguniform('xgb_learning_rate', np.log(0.01), np.log(0.3)),
            'xgb_max_depth': hp.choice('xgb_max_depth', [3, 4, 5, 6, 7, 8, 10]),
            'xgb_min_child_weight': hp.choice('xgb_min_child_weight', [1, 3, 5, 7]),
            'xgb_subsample': hp.uniform('xgb_subsample', 0.6, 1.0),
            'xgb_colsample_bytree': hp.uniform('xgb_colsample_bytree', 0.6, 1.0),
            'xgb_reg_alpha': hp.loguniform('xgb_reg_alpha', np.log(0.01), np.log(10)),
            'xgb_reg_lambda': hp.loguniform('xgb_reg_lambda', np.log(0.01), np.log(10)),
            'xgb_gamma': hp.loguniform('xgb_gamma', np.log(0.01), np.log(1)),
            
            # Meta Model è¶…åƒæ•¸
            'meta_C': hp.loguniform('meta_C', np.log(0.001), np.log(100)),
            'meta_solver': hp.choice('meta_solver', ['liblinear', 'lbfgs'])
        }
        
        # æ•¸æ“šé è™•ç† - æ¸›å°‘ pickle å‚³è¼¸
        # åˆ†å±¤æŠ½æ¨£ï¼Œç¢ºä¿é¡åˆ¥å¹³è¡¡ä¸”æ¸›å°‘æ•¸æ“šé‡
        sample_size = min(3000, len(X))
        
        # ç¢ºä¿ train_size ä¸æœƒæ˜¯ 1.0
        train_ratio = min(0.99, sample_size / len(X))
        
        if train_ratio < 1.0:
            X_sample, _, y_sample, _ = train_test_split(
                X, y, 
                train_size=train_ratio, 
                stratify=y, 
                random_state=42
            )
        else:
            # å¦‚æœæ¨£æœ¬æ•¸é‡å¾ˆå°ï¼Œç›´æ¥ä½¿ç”¨å…¨éƒ¨æ•¸æ“š
            X_sample, y_sample = X, y
        
        logger.info(f"ä½¿ç”¨ {len(X_sample)} å€‹æ¨£æœ¬é€²è¡Œè¶…åƒæ•¸å„ªåŒ–ï¼ˆåŸå§‹: {len(X)}ï¼‰")
        
        # å„ªåŒ–çš„ç›®æ¨™å‡½æ•¸
        def objective(params):
            """å„ªåŒ–å¾Œçš„ HyperOpt ç›®æ¨™å‡½æ•¸"""
            try:
                # è¨­ç½®éš¨æ©Ÿç¨®å­ç¢ºä¿å¯é‡ç¾æ€§
                trial_seed = 42 + getattr(objective, 'trial_count', 0)
                np.random.seed(trial_seed)
                
                # å‰µå»º LightGBM æ¨¡å‹
                lgbm_params = {
                    'n_estimators': params['lgbm_n_estimators'],
                    'learning_rate': params['lgbm_learning_rate'],
                    'num_leaves': params['lgbm_num_leaves'],
                    'max_depth': params['lgbm_max_depth'],
                    'min_child_samples': params['lgbm_min_child_samples'],
                    'subsample': params['lgbm_subsample'],
                    'colsample_bytree': params['lgbm_colsample_bytree'],
                    'reg_alpha': params['lgbm_reg_alpha'],
                    'reg_lambda': params['lgbm_reg_lambda'],
                    'random_state': trial_seed,
                    'verbosity': -1,
                    'class_weight': 'balanced',
                    'n_jobs': 1,  # é¿å…éåº¦ä¸¦è¡ŒåŒ–
                    'device_type': 'cpu',  # æˆ– 'gpu' å¦‚æœæœ‰ GPU
                    'force_row_wise': True  # é¿å…è­¦å‘Š
                }
                
                lgbm = lgb.LGBMClassifier(**lgbm_params)
                
                # å‰µå»º XGBoost æ¨¡å‹
                xgb_params = {
                    'n_estimators': params['xgb_n_estimators'],
                    'learning_rate': params['xgb_learning_rate'],
                    'max_depth': params['xgb_max_depth'],
                    'min_child_weight': params['xgb_min_child_weight'],
                    'subsample': params['xgb_subsample'],
                    'colsample_bytree': params['xgb_colsample_bytree'],
                    'reg_alpha': params['xgb_reg_alpha'],
                    'reg_lambda': params['xgb_reg_lambda'],
                    'gamma': params['xgb_gamma'],
                    'random_state': trial_seed,
                    'verbosity': 0,
                    'use_label_encoder': False,
                    'eval_metric': 'logloss',
                    'n_jobs': 1,  # é¿å…éåº¦ä¸¦è¡ŒåŒ–
                    'tree_method': 'hist'  # æˆ– 'gpu_hist' å¦‚æœæœ‰ GPU
                }
                
                xgb_model = xgb.XGBClassifier(**xgb_params)
                
                # Meta Model
                meta_params = {
                    'C': params['meta_C'],
                    'solver': params['meta_solver'],
                    'random_state': trial_seed,
                    'max_iter': 1000,
                    'n_jobs': 1
                }
                
                meta_model = LogisticRegression(**meta_params)
                
                # å‰µå»º Stacking åˆ†é¡å™¨
                stack = StackingClassifier(
                    estimators=[('lgbm', lgbm), ('xgb', xgb_model)],
                    final_estimator=meta_model,
                    passthrough=False,
                    cv=StratifiedKFold(n_splits=3, shuffle=True, random_state=trial_seed),
                    n_jobs=1,  # é¿å…éåº¦ä¸¦è¡ŒåŒ–
                    verbose=0
                )
                
                # äº¤å‰é©—è­‰è©•ä¼°
                cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=trial_seed)
                scores = cross_val_score(
                    stack, X_sample, y_sample, 
                    cv=cv, 
                    scoring='roc_auc',
                    n_jobs=1,  # é¿å…éåº¦ä¸¦è¡ŒåŒ–
                    verbose=0
                )
                
                score = scores.mean()
                score_std = scores.std()
                
                # æ›´æ–°é€²åº¦
                current_trial = getattr(objective, 'trial_count', 0) + 1
                setattr(objective, 'trial_count', current_trial)
                
                if training_jobs:
                    progress = min(90, 30 + int(60 * current_trial / n_trials))
                    training_jobs[job_id]["progress"] = progress
                    training_jobs[job_id]["message"] = f"Trial {current_trial}/{n_trials} - AUC: {score:.4f}Â±{score_std:.4f}"
                
                # è¨˜éŒ„è©³ç´°ä¿¡æ¯
                logger.info(f"Trial {current_trial}: AUC = {score:.4f}Â±{score_std:.4f}")
                
                # ç¢ºä¿åˆ†æ•¸æœ‰æ„ç¾©çš„è®ŠåŒ–
                if score < 0.5:  # å¦‚æœåˆ†æ•¸å¤ªä½ï¼Œå¯èƒ½æœ‰å•é¡Œ
                    logger.warning(f"Trial {current_trial}: ç•°å¸¸ä½åˆ†æ•¸ {score:.4f}")
                
                return {
                    'loss': -score,  # æœ€å°åŒ–è²  AUC = æœ€å¤§åŒ– AUC
                    'status': STATUS_OK,
                    'eval_time': datetime.now(),
                    'score_std': score_std,
                    'params_hash': hash(str(sorted(params.items())))  # åƒæ•¸å”¯ä¸€æ€§æª¢æŸ¥
                }
                
            except Exception as e:
                logger.error(f"Trial {getattr(objective, 'trial_count', 0) + 1} åŸ·è¡ŒéŒ¯èª¤: {str(e)}")
                return {'loss': 1.0, 'status': STATUS_OK}  # å°æ‡‰ AUC = 0
        
        # è¼‰å…¥æˆ–å‰µå»º Trials ç‰©ä»¶
        os.makedirs('models', exist_ok=True)
        trials_save_file = f'models/hyperopt_trials_{job_id}.pkl'
        
        if os.path.exists(trials_save_file):
            logger.info("è¼‰å…¥ç¾æœ‰çš„ trials æ­·å²...")
            try:
                with open(trials_save_file, 'rb') as f:
                    trials = pickle.load(f)
                logger.info(f"è¼‰å…¥äº† {len(trials.trials)} å€‹æ­·å²è©¦é©—")
            except Exception as e:
                logger.warning(f"è¼‰å…¥ trials å¤±æ•—: {e}ï¼Œå‰µå»ºæ–°çš„ trials")
                trials = Trials()
        else:
            trials = Trials()
        
        # æª¢æŸ¥æ­·å²è©¦é©—çš„åˆ†æ•¸åˆ†å¸ƒ
        if trials.trials:
            trial_losses = [trial['result']['loss'] for trial in trials.trials if 'result' in trial]
            if trial_losses:
                logger.info(f"æ­·å²è©¦é©—åˆ†æ•¸ç¯„åœ: {min(trial_losses):.4f} åˆ° {max(trial_losses):.4f}")
                if len(set(trial_losses)) == 1:
                    logger.warning("æ‰€æœ‰æ­·å²è©¦é©—åˆ†æ•¸ç›¸åŒï¼Œå¯èƒ½å­˜åœ¨å•é¡Œï¼")
        
        # ä½¿ç”¨ TPE é€²è¡Œè¶…åƒæ•¸èª¿æ•´
        logger.info(f"é–‹å§‹ HyperOpt å„ªåŒ–ï¼Œç›®æ¨™è©¦é©—æ¬¡æ•¸: {n_trials}ï¼Œç•¶å‰å·²æœ‰: {len(trials.trials)}")
        
        remaining_trials = max(0, n_trials - len(trials.trials))
        
        if remaining_trials > 0:
            best = fmin(
                fn=objective,
                space=space,
                algo=tpe.suggest,
                max_evals=n_trials,
                trials=trials,
                verbose=True,
                rstate=np.random.default_rng(42),  # ä½¿ç”¨æ–°çš„éš¨æ©Ÿæ•¸ç”Ÿæˆå™¨
                show_progressbar=False  # é¿å…èˆ‡ tqdm è¡çª
            )
            
            # å„²å­˜ trials æ­·å²
            try:
                with open(trials_save_file, 'wb') as f:
                    pickle.dump(trials, f)
                logger.info(f"å·²å„²å­˜ trials æ­·å²åˆ° {trials_save_file}")
            except Exception as e:
                logger.warning(f"å„²å­˜ trials å¤±æ•—: {e}")
        else:
            logger.info("å·²é”åˆ°ç›®æ¨™è©¦é©—æ¬¡æ•¸ï¼Œä½¿ç”¨ç¾æœ‰æœ€ä½³çµæœ")
            best = trials.argmin
        
        # åˆ†æå„ªåŒ–çµæœ
        if trials.trials:
            trial_losses = [trial['result']['loss'] for trial in trials.trials if 'result' in trial]
            if trial_losses:
                best_score = -min(trial_losses)
                worst_score = -max(trial_losses)
                logger.info(f"å„ªåŒ–çµæœçµ±è¨ˆ:")
                logger.info(f"  æœ€ä½³ AUC: {best_score:.4f}")
                logger.info(f"  æœ€å·® AUC: {worst_score:.4f}")
                logger.info(f"  æ”¹é€²å¹…åº¦: {best_score - worst_score:.4f}")
                logger.info(f"  ç¸½è©¦é©—æ•¸: {len(trial_losses)}")
                logger.info(f"  ä¸åŒåˆ†æ•¸æ•¸é‡: {len(set(trial_losses))}")
                
                # æª¢æŸ¥æ˜¯å¦æœ‰æœ‰æ•ˆçš„æ¢ç´¢
                if len(set(trial_losses)) == 1:
                    logger.warning("âš ï¸  æ‰€æœ‰è©¦é©—åˆ†æ•¸ç›¸åŒï¼Œè¶…åƒæ•¸å„ªåŒ–å¯èƒ½æ²’æœ‰ç”Ÿæ•ˆï¼")
                elif len(set(trial_losses)) < len(trial_losses) * 0.5:
                    logger.warning("âš ï¸  å¤§éƒ¨åˆ†è©¦é©—åˆ†æ•¸ç›¸åŒï¼Œæœç´¢ç©ºé–“å¯èƒ½éœ€è¦èª¿æ•´")
                else:
                    logger.info("âœ…  è©¦é©—åˆ†æ•¸æœ‰è‰¯å¥½çš„å¤šæ¨£æ€§ï¼Œå„ªåŒ–æœ‰æ•ˆ")
            
            best_trial = trials.best_trial
            best_score = -best_trial['result']['loss']
            
            # è™•ç†æœ€ä½³åƒæ•¸
            best_params = best_trial['misc']['vals']
            best_params_formatted = {}
            
            # åƒæ•¸è½‰æ›æ˜ å°„
            choice_mappings = {
                'lgbm_n_estimators': [100, 200, 300, 500, 800, 1000],
                'lgbm_num_leaves': [31, 63, 127, 255, 511],
                'lgbm_max_depth': [-1, 3, 5, 7, 9, 12, 15],
                'lgbm_min_child_samples': [10, 20, 50, 100],
                'xgb_n_estimators': [100, 200, 300, 500, 800, 1000],
                'xgb_max_depth': [3, 4, 5, 6, 7, 8, 10],
                'xgb_min_child_weight': [1, 3, 5, 7],
                'meta_solver': ['liblinear', 'lbfgs']
            }
            
            for key, value in best_params.items():
                if isinstance(value, list) and len(value) > 0:
                    if key in choice_mappings:
                        choice_index = value[0]
                        best_params_formatted[key] = choice_mappings[key][choice_index]
                    else:
                        best_params_formatted[key] = value[0]
            
            # æ·»åŠ æ¨¡å‹é¡å‹
            best_params_formatted['model_type'] = 'stacking'
            
            # å‰µå»ºçµæœå­—å…¸
            result = {
                "best_params": best_params_formatted,
                "best_score": float(best_score),
                "n_trials": len(trials.trials),
                "optimization_method": "HyperOpt_TPE_Enhanced",
                "trials_file": trials_save_file,
                "completed_at": datetime.now().isoformat(),
                "score_variance": float(best_trial['result'].get('score_std', 0)),
                "exploration_diversity": len(set(trial_losses)) / len(trial_losses) if trial_losses else 0
            }
            
            if training_jobs:
                training_jobs[job_id]["status"] = "SUCCESS"
                training_jobs[job_id]["progress"] = 100
                training_jobs[job_id]["message"] = f"å„ªåŒ–å®Œæˆï¼æœ€ä½³ AUC: {best_score:.4f} ({len(trials.trials)} trials)"
                training_jobs[job_id]["result"] = result
            
            logger.info(f"âœ… HyperOpt è¶…åƒæ•¸å„ªåŒ–ä»»å‹™ {job_id} æˆåŠŸå®Œæˆ")
            logger.info(f"ğŸ“Š æœ€ä½³åˆ†æ•¸: {best_score:.4f}")
            logger.info(f"ğŸ”¬ ç¸½è©¦é©—æ¬¡æ•¸: {len(trials.trials)}")
            logger.info(f"ğŸ¯ æœ€ä½³åƒæ•¸: {best_params_formatted}")
            
            # è¿”å›å„ªåŒ–çµæœ
            return result
            
        else:
            raise Exception("æ²’æœ‰æˆåŠŸçš„è©¦é©—")
        
    except Exception as e:
        error_msg = f"HyperOpt è¶…åƒæ•¸å„ªåŒ–å¤±æ•—: {str(e)}"
        logger.error(f"âŒ ä»»å‹™ {job_id} - {error_msg}")
        
        if training_jobs:
            training_jobs[job_id]["status"] = "FAILURE"
            training_jobs[job_id]["message"] = error_msg
            training_jobs[job_id]["error"] = str(e)


@celery.task
def train_model_task(
    job_id: str, 
    df: pd.DataFrame, 
    use_hyperopt: bool = False,
    cv_folds: int = 5,
    training_jobs: Dict[str, Any] = None
):
    """
    å„ªåŒ–å¾Œçš„æ¨¡å‹è¨“ç·´ä»»å‹™
    
    Args:
        job_id: ä»»å‹™ ID
        df: è¨“ç·´æ•¸æ“š
        use_hyperopt: æ˜¯å¦ä½¿ç”¨è¶…åƒæ•¸å„ªåŒ–
        cv_folds: äº¤å‰é©—è­‰æŠ˜æ•¸
        training_jobs: ä»»å‹™ç‹€æ…‹å­—å…¸
    """
    try:
        logger.info(f"ğŸš€ é–‹å§‹åŸ·è¡Œè¨“ç·´ä»»å‹™: {job_id}")
        
        # å‰µå»ºç¸½é«”ä»»å‹™é€²åº¦æ¢
        task_pbar = tqdm(
            total=6,
            desc="ğŸš€ æ¨¡å‹è¨“ç·´ç®¡ç·š",
            unit="éšæ®µ",
            ncols=100,
            file=sys.stdout
        )
        
        try:
            # éšæ®µ 1: æ•¸æ“šé è™•ç†
            task_pbar.set_description("ğŸ”§ æ•¸æ“šé è™•ç†éšæ®µ")
            if training_jobs:
                training_jobs[job_id]["status"] = "PROCESSING"
                training_jobs[job_id]["progress"] = 10
                training_jobs[job_id]["current_step"] = "Data Preprocessing"
                training_jobs[job_id]["message"] = "æ­£åœ¨é è™•ç†æ•¸æ“š..."
            
            # æ•¸æ“šé è™•ç†
            preprocessor = AdvancedDataPreprocessor(create_interactions=True)
            X, y = preprocessor.fit_transform_with_target(df)
            
            logger.info(f"âœ… æ•¸æ“šé è™•ç†å®Œæˆï¼Œç‰¹å¾µæ•¸é‡: {X.shape[1]}")
            task_pbar.update(1)
            
            # éšæ®µ 2: è¶…åƒæ•¸å„ªåŒ–ï¼ˆå¦‚æœå•Ÿç”¨ï¼‰
            optimized_params = None
            if use_hyperopt:
                task_pbar.set_description("âš™ï¸ è¶…åƒæ•¸å„ªåŒ–éšæ®µ")
                if training_jobs:
                    training_jobs[job_id]["progress"] = 20
                    training_jobs[job_id]["current_step"] = "Hyperparameter Optimization"
                    training_jobs[job_id]["message"] = "æ­£åœ¨é€²è¡Œè¶…åƒæ•¸å„ªåŒ–..."
                
                logger.info("ğŸ”¬ ä½¿ç”¨ HyperOpt å„ªåŒ–è¶…åƒæ•¸...")
                
                # åŸ·è¡Œè¶…åƒæ•¸å„ªåŒ–ä¸¦ç²å–çµæœ
                hyperopt_job_id = f"{job_id}_hyperopt"
                
                # åˆå§‹åŒ–è¶…åƒæ•¸å„ªåŒ–ä»»å‹™ç‹€æ…‹
                if training_jobs:
                    training_jobs[hyperopt_job_id] = {
                        "status": "PROCESSING",
                        "created_at": datetime.now().isoformat(),
                        "progress": 0,
                        "message": "é–‹å§‹è¶…åƒæ•¸å„ªåŒ–..."
                    }
                
                # ç›´æ¥èª¿ç”¨è¶…åƒæ•¸å„ªåŒ–å‡½æ•¸ä¸¦ç²å–çµæœ
                optimize_result = optimize_hyperparameters_task(
                    hyperopt_job_id, X, y, n_trials=50, training_jobs=training_jobs
                )
                
                # æª¢æŸ¥å„ªåŒ–çµæœ
                if optimize_result and 'best_params' in optimize_result:
                    optimized_params = optimize_result['best_params']
                    logger.info(f"âœ… è¶…åƒæ•¸å„ªåŒ–å®Œæˆ: {optimized_params}")
                else:
                    logger.warning("âš ï¸ è¶…åƒæ•¸å„ªåŒ–å¤±æ•—ï¼Œå°‡ä½¿ç”¨é è¨­åƒæ•¸")
                
                task_pbar.update(1)
            else:
                logger.info("â­ï¸  è·³éè¶…åƒæ•¸å„ªåŒ–éšæ®µ")
                task_pbar.update(1)  # è·³éè¶…åƒæ•¸å„ªåŒ–éšæ®µ
            
            # éšæ®µ 3: æ¨¡å‹é…ç½®
            task_pbar.set_description("ğŸ”§ é…ç½®æ¨¡å‹æ¶æ§‹")
            if training_jobs:
                training_jobs[job_id]["progress"] = 40
                training_jobs[job_id]["current_step"] = "Model Configuration"
                training_jobs[job_id]["message"] = "æ­£åœ¨é…ç½®æ¨¡å‹æ¶æ§‹..."
            
            # å‰µå»ºå’Œé…ç½®æ¨¡å‹
            stacking_model = StackingModel(cv_folds=cv_folds)
            
            if optimized_params:
                logger.info(f"ğŸ¯ ä½¿ç”¨å„ªåŒ–å¾Œçš„è¶…åƒæ•¸: {optimized_params}")
                
                # æ›´æ–° LightGBM åƒæ•¸
                lgb_updates = {}
                for key, value in optimized_params.items():
                    if key.startswith('lgbm_'):
                        param_name = key.replace('lgbm_', '')
                        if param_name in ['n_estimators', 'num_leaves', 'max_depth', 
                                         'min_child_samples']:
                            lgb_updates[param_name] = int(value)
                        else:
                            lgb_updates[param_name] = value
                
                if lgb_updates:
                    stacking_model.lgb_params.update(lgb_updates)
                    logger.info(f"ğŸ“Š LightGBM åƒæ•¸å·²æ›´æ–°: {lgb_updates}")
                
                # æ›´æ–° XGBoost åƒæ•¸
                xgb_updates = {}
                for key, value in optimized_params.items():
                    if key.startswith('xgb_'):
                        param_name = key.replace('xgb_', '')
                        if param_name in ['n_estimators', 'max_depth', 'min_child_weight']:
                            xgb_updates[param_name] = int(value)
                        else:
                            xgb_updates[param_name] = value
                
                if xgb_updates:
                    stacking_model.xgb_params.update(xgb_updates)
                    logger.info(f"ğŸš€ XGBoost åƒæ•¸å·²æ›´æ–°: {xgb_updates}")
                
                # æ›´æ–° Meta Model
                if 'meta_C' in optimized_params:
                    stacking_model.meta_model = LogisticRegression(
                        C=optimized_params['meta_C'],
                        solver=optimized_params.get('meta_solver', 'lbfgs'),
                        random_state=42,
                        max_iter=1000
                    )
                    logger.info(f"ğŸ¯ Meta Model åƒæ•¸å·²æ›´æ–°: C={optimized_params['meta_C']}")
            else:
                logger.info("â„¹ï¸ ä½¿ç”¨é è¨­è¶…åƒæ•¸")
            
            task_pbar.update(1)
            
            # éšæ®µ 4: æ¨¡å‹è¨“ç·´
            task_pbar.set_description("ğŸš€ æ¨¡å‹è¨“ç·´éšæ®µ")
            if training_jobs:
                training_jobs[job_id]["progress"] = 60
                training_jobs[job_id]["current_step"] = "Model Training"
                training_jobs[job_id]["message"] = "æ­£åœ¨è¨“ç·´æ¨¡å‹..."
            
            # è¨“ç·´æ¨¡å‹
            stacking_model.fit(X, y)
            
            logger.info("âœ… æ¨¡å‹è¨“ç·´å®Œæˆ")
            task_pbar.update(1)
            
            # éšæ®µ 5: ä¿å­˜æ¨¡å‹
            task_pbar.set_description("ğŸ’¾ ä¿å­˜æ¨¡å‹")
            if training_jobs:
                training_jobs[job_id]["progress"] = 80
                training_jobs[job_id]["current_step"] = "Model Saving"
                training_jobs[job_id]["message"] = "æ­£åœ¨ä¿å­˜æ¨¡å‹..."
            
            # ç”Ÿæˆæ¨¡å‹ ID ä¸¦ä¿å­˜
            model_id = generate_model_id()
            save_model(stacking_model, preprocessor, model_id)
            
            logger.info(f"âœ… æ¨¡å‹å·²ä¿å­˜ï¼Œæ¨¡å‹ ID: {model_id}")
            task_pbar.update(1)
            
            # éšæ®µ 6: æ€§èƒ½è©•ä¼°
            task_pbar.set_description("ğŸ“Š æ€§èƒ½è©•ä¼°")
            if training_jobs:
                training_jobs[job_id]["progress"] = 90
                training_jobs[job_id]["current_step"] = "Performance Evaluation"
                training_jobs[job_id]["message"] = "æ­£åœ¨è©•ä¼°æ¨¡å‹æ€§èƒ½..."
            
            # è¨ˆç®—äº¤å‰é©—è­‰åˆ†æ•¸
            cv_scores = stacking_model.get_cv_scores()
            
            # æ›´æ–°ä»»å‹™å®Œæˆç‹€æ…‹
            result = {
                "model_id": model_id,
                "cv_scores": cv_scores,
                "feature_count": X.shape[1],
                "training_samples": len(X),
                "hyperopt_used": use_hyperopt,
                "optimized_params": optimized_params,  # æ·»åŠ å„ªåŒ–åƒæ•¸
                "cv_folds": cv_folds,
                "completed_at": datetime.now().isoformat()
            }
            
            if training_jobs:
                training_jobs[job_id]["status"] = "SUCCESS"
                training_jobs[job_id]["progress"] = 100
                training_jobs[job_id]["current_step"] = "Completed"
                training_jobs[job_id]["message"] = f"âœ… è¨“ç·´å®Œæˆï¼æ¨¡å‹ ID: {model_id}"
                training_jobs[job_id]["result"] = result
            
            task_pbar.update(1)
            logger.info(f"ğŸ‰ æ¨¡å‹è¨“ç·´ä»»å‹™ {job_id} å®Œæˆï¼Œæ¨¡å‹ ID: {model_id}")
            
        finally:
            task_pbar.close()
            
    except Exception as e:
        error_msg = f"æ¨¡å‹è¨“ç·´å¤±æ•—: {str(e)}"
        logger.error(f"âŒ ä»»å‹™ {job_id} - {error_msg}")
        
        if training_jobs:
            training_jobs[job_id]["status"] = "FAILURE"
            training_jobs[job_id]["message"] = error_msg
            training_jobs[job_id]["error"] = str(e)
        
        # ç¢ºä¿é€²åº¦æ¢é—œé–‰
        try:
            task_pbar.close()
        except:
            pass


@celery.task
def batch_prediction_task(
    job_id: str,
    model_id: str,
    input_file: str,
    output_file: str,
    training_jobs: Dict[str, Any] = None
):
    """
    æ‰¹æ¬¡é æ¸¬ä»»å‹™
    
    Args:
        job_id: ä»»å‹™ ID
        model_id: æ¨¡å‹ ID
        input_file: è¼¸å…¥æª”æ¡ˆè·¯å¾‘
        output_file: è¼¸å‡ºæª”æ¡ˆè·¯å¾‘
        training_jobs: ä»»å‹™ç‹€æ…‹å­—å…¸
    """
    try:
        from .utils import load_model
        
        logger.info(f"ğŸ”® é–‹å§‹åŸ·è¡Œæ‰¹æ¬¡é æ¸¬ä»»å‹™: {job_id}")
        
        if training_jobs:
            training_jobs[job_id]["status"] = "PROCESSING"
            training_jobs[job_id]["progress"] = 10
            training_jobs[job_id]["message"] = "æ­£åœ¨è¼‰å…¥æ¨¡å‹..."
        
        # è¼‰å…¥æ¨¡å‹
        model, preprocessor = load_model(model_id)
        
        if training_jobs:
            training_jobs[job_id]["progress"] = 30
            training_jobs[job_id]["message"] = "æ­£åœ¨è¼‰å…¥é æ¸¬æ•¸æ“š..."
        
        # è¼‰å…¥é æ¸¬æ•¸æ“š
        df = pd.read_csv(input_file)
        
        if training_jobs:
            training_jobs[job_id]["progress"] = 50
            training_jobs[job_id]["message"] = "æ­£åœ¨é€²è¡Œæ•¸æ“šé è™•ç†..."
        
        # é è™•ç†æ•¸æ“š
        X = preprocessor.transform(df)
        
        if training_jobs:
            training_jobs[job_id]["progress"] = 70
            training_jobs[job_id]["message"] = "æ­£åœ¨é€²è¡Œé æ¸¬..."
        
        # é€²è¡Œé æ¸¬
        predictions = model.predict_proba(X)[:, 1]
        
        if training_jobs:
            training_jobs[job_id]["progress"] = 90
            training_jobs[job_id]["message"] = "æ­£åœ¨ä¿å­˜é æ¸¬çµæœ..."
        
        # ä¿å­˜é æ¸¬çµæœ
        result_df = df.copy()
        result_df['prediction'] = predictions
        result_df.to_csv(output_file, index=False)
        
        # æ›´æ–°ä»»å‹™ç‹€æ…‹
        result = {
            "model_id": model_id,
            "input_file": input_file,
            "output_file": output_file,
            "prediction_count": len(predictions),
            "completed_at": datetime.now().isoformat()
        }
        
        if training_jobs:
            training_jobs[job_id]["status"] = "SUCCESS"
            training_jobs[job_id]["progress"] = 100
            training_jobs[job_id]["message"] = f"âœ… é æ¸¬å®Œæˆï¼è™•ç†äº† {len(predictions)} ç­†æ•¸æ“š"
            training_jobs[job_id]["result"] = result
        
        logger.info(f"ğŸ‰ æ‰¹æ¬¡é æ¸¬ä»»å‹™ {job_id} å®Œæˆ")
        
    except Exception as e:
        error_msg = f"æ‰¹æ¬¡é æ¸¬å¤±æ•—: {str(e)}"
        logger.error(f"âŒ ä»»å‹™ {job_id} - {error_msg}")
        
        if training_jobs:
            training_jobs[job_id]["status"] = "FAILURE"
            training_jobs[job_id]["message"] = error_msg
            training_jobs[job_id]["error"] = str(e)
