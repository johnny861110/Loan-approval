"""
Stacking æ¨¡å‹ - LightGBM + XGBoost + Logistic Meta Model
å„ªåŒ–ç‰ˆæœ¬ï¼šæ”¯æŒ GPU åŠ é€Ÿã€æ”¹é€²çš„é€²åº¦æ¢ã€æ›´å¥½çš„éŒ¯èª¤è™•ç†
"""

import numpy as np
import pandas as pd
import warnings
from sklearn.model_selection import StratifiedKFold
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, roc_auc_score, classification_report
import lightgbm as lgb
import xgboost as xgb
import shap
import pickle
import logging
from typing import Tuple, List, Dict, Any
from tqdm import tqdm
import sys

# æŠ‘åˆ¶è­¦å‘Š
warnings.filterwarnings("ignore", category=UserWarning, module="lightgbm")
warnings.filterwarnings("ignore", category=UserWarning, module="xgboost")

logger = logging.getLogger(__name__)

class StackingModel:
    """
    LightGBM + XGBoost Stacking æ¨¡å‹
    ä½¿ç”¨ Logistic Regression ä½œç‚º Meta Model
    """
    
    def __init__(self, cv_folds: int = 5, random_state: int = 42):
        self.cv_folds = cv_folds
        self.random_state = random_state
        
        # Base Models - æ·»åŠ  GPU æ”¯æŒæª¢æ¸¬
        self.lgb_params = {
            'objective': 'binary',
            'metric': 'binary_logloss',
            'boosting_type': 'gbdt',
            'num_leaves': 31,
            'learning_rate': 0.1,
            'feature_fraction': 0.9,
            'bagging_fraction': 0.8,
            'bagging_freq': 5,
            'verbose': -1,
            'random_state': random_state,
            'class_weight': 'balanced',
            'device_type': 'cpu',  # å¯ä»¥æ”¹ç‚º 'gpu' å¦‚æœæœ‰ GPU
            'force_row_wise': True,  # é¿å…è­¦å‘Š
            'num_threads': -1
        }
        
        self.xgb_params = {
            'objective': 'binary:logistic',
            'eval_metric': 'logloss',
            'max_depth': 6,
            'learning_rate': 0.1,
            'n_estimators': 100,
            'subsample': 0.8,
            'colsample_bytree': 0.9,
            'random_state': random_state,
            'verbosity': 0,
            'scale_pos_weight': 1,  # å¯æ ¹æ“šæ•¸æ“šä¸å¹³è¡¡èª¿æ•´
            'tree_method': 'hist',  # å¯ä»¥æ”¹ç‚º 'gpu_hist' å¦‚æœæœ‰ GPU
            'n_jobs': -1,
            'use_label_encoder': False
        }
        
        # Meta Model
        self.meta_model = LogisticRegression(random_state=random_state)
        
        # å­˜å„²è¨“ç·´å¥½çš„æ¨¡å‹
        self.base_models = {'lgb': [], 'xgb': []}
        self.is_fitted = False
        
        # å­˜å„²äº¤å‰é©—è­‰åˆ†æ•¸
        self.cv_scores = {}
        
        # SHAP è§£é‡‹å™¨
        self.shap_explainer = None
        self.global_shap_values = None
        
    def update_hyperparameters(self, optimized_params: Dict[str, Any]):
        """
        æ›´æ–°è¶…åƒæ•¸ï¼ˆä¾†è‡ª HyperOpt å„ªåŒ–çµæœï¼‰
        
        Args:
            optimized_params: HyperOpt å„ªåŒ–å¾Œçš„åƒæ•¸å­—å…¸
        """
        logger.info("æ­£åœ¨æ›´æ–°æ¨¡å‹è¶…åƒæ•¸...")
        
        # æ›´æ–° LightGBM åƒæ•¸
        lgb_updates = {}
        for key, value in optimized_params.items():
            if key.startswith('lgbm_'):
                param_name = key.replace('lgbm_', '')
                lgb_updates[param_name] = value
        
        if lgb_updates:
            self.lgb_params.update(lgb_updates)
            logger.info(f"LightGBM åƒæ•¸å·²æ›´æ–°: {lgb_updates}")
        
        # æ›´æ–° XGBoost åƒæ•¸
        xgb_updates = {}
        for key, value in optimized_params.items():
            if key.startswith('xgb_'):
                param_name = key.replace('xgb_', '')
                xgb_updates[param_name] = value
        
        if xgb_updates:
            self.xgb_params.update(xgb_updates)
            logger.info(f"XGBoost åƒæ•¸å·²æ›´æ–°: {xgb_updates}")
        
        # æ›´æ–° Meta Model åƒæ•¸
        if 'meta_C' in optimized_params:
            self.meta_model = LogisticRegression(
                C=optimized_params['meta_C'],
                solver=optimized_params.get('meta_solver', 'lbfgs'),
                random_state=self.random_state,
                max_iter=1000
            )
            logger.info(f"Meta Model åƒæ•¸å·²æ›´æ–°: C={optimized_params['meta_C']}")
        
        return self
        
    def _train_base_models(self, X: pd.DataFrame, y: pd.Series) -> np.ndarray:
        """
        è¨“ç·´ Base Models ä¸¦ç”¢ç”Ÿ Out-of-Fold é æ¸¬
        
        Args:
            X: ç‰¹å¾µæ•¸æ“š
            y: ç›®æ¨™è®Šæ•¸
            
        Returns:
            Out-of-fold é æ¸¬çµæœ (n_samples, n_base_models)
        """
        n_samples = len(X)
        oof_predictions = np.zeros((n_samples, 2))  # LightGBM + XGBoost
        
        # è¨­å®šäº¤å‰é©—è­‰
        skf = StratifiedKFold(n_splits=self.cv_folds, shuffle=True, random_state=self.random_state)
        
        logger.info(f"é–‹å§‹ {self.cv_folds}-Fold äº¤å‰é©—è­‰è¨“ç·´")
        
        # å‰µå»ºé€²åº¦æ¢
        fold_pbar = tqdm(
            enumerate(skf.split(X, y)), 
            total=self.cv_folds,
            desc="ğŸ”„ äº¤å‰é©—è­‰é€²åº¦",
            unit="æŠ˜",
            ncols=100,
            file=sys.stdout
        )
        
        for fold, (train_idx, val_idx) in fold_pbar:
            fold_pbar.set_description(f"ğŸ”„ è¨“ç·´ç¬¬ {fold + 1}/{self.cv_folds} æŠ˜")
            logger.info(f"è¨“ç·´ç¬¬ {fold + 1} æŠ˜...")
            
            X_train_fold, X_val_fold = X.iloc[train_idx], X.iloc[val_idx]
            y_train_fold, y_val_fold = y.iloc[train_idx], y.iloc[val_idx]
            
            # è¨“ç·´ LightGBM
            fold_pbar.set_postfix_str("ğŸ“Š è¨“ç·´ LightGBM...")
            lgb_train = lgb.Dataset(X_train_fold, y_train_fold)
            lgb_val = lgb.Dataset(X_val_fold, y_val_fold, reference=lgb_train)
            
            lgb_model = lgb.train(
                self.lgb_params,
                lgb_train,
                num_boost_round=100,
                valid_sets=[lgb_val],
                callbacks=[lgb.early_stopping(stopping_rounds=10), lgb.log_evaluation(0)]
            )
            
            # LightGBM é æ¸¬
            lgb_val_pred = lgb_model.predict(X_val_fold, num_iteration=lgb_model.best_iteration)
            oof_predictions[val_idx, 0] = lgb_val_pred
            
            # ä¿å­˜ LightGBM æ¨¡å‹
            self.base_models['lgb'].append(lgb_model)
            
            # è¨“ç·´ XGBoost
            fold_pbar.set_postfix_str("âš¡ è¨“ç·´ XGBoost...")
            xgb_init_params = dict(self.xgb_params)
            
            # ä½¿ç”¨ XGBoost ä½å±¤ç´š API ä»¥æ”¯æ´ early stopping
            dtrain = xgb.DMatrix(X_train_fold, label=y_train_fold)
            dval = xgb.DMatrix(X_val_fold, label=y_val_fold)
            
            # è¨­ç½® XGBoost è¨“ç·´åƒæ•¸
            params = {
                'objective': 'binary:logistic',
                'eval_metric': 'logloss',
                'max_depth': self.xgb_params.get('max_depth', 6),
                'learning_rate': self.xgb_params.get('learning_rate', 0.1),
                'subsample': self.xgb_params.get('subsample', 0.8),
                'colsample_bytree': self.xgb_params.get('colsample_bytree', 0.9),
                'random_state': self.xgb_params.get('random_state', 42),
                'verbosity': 0,
                'scale_pos_weight': self.xgb_params.get('scale_pos_weight', 1)
            }
            
            # ç›´æ¥ä½¿ç”¨ XGBClassifier é€²è¡Œè¨“ç·´
            xgb_model = xgb.XGBClassifier(**xgb_init_params)
            
            # æ¨™æº– fit æ–¹æ³•ï¼Œä½¿ç”¨ eval_set é€²è¡Œæ—©æœŸåœæ­¢
            xgb_model.fit(
                X_train_fold, y_train_fold,
                eval_set=[(X_val_fold, y_val_fold)],
                verbose=False
            )
            
            
            # XGBoost é æ¸¬
            xgb_val_pred = xgb_model.predict_proba(X_val_fold)[:, 1]
            oof_predictions[val_idx, 1] = xgb_val_pred
            
            # ä¿å­˜ XGBoost æ¨¡å‹
            self.base_models['xgb'].append(xgb_model)
            
            # è¨ˆç®— Fold æ€§èƒ½
            fold_pred = (lgb_val_pred + xgb_val_pred) / 2
            fold_auc = roc_auc_score(y_val_fold, fold_pred)
            lgb_auc = roc_auc_score(y_val_fold, lgb_val_pred)
            xgb_auc = roc_auc_score(y_val_fold, xgb_val_pred)
            logger.info(f"ç¬¬ {fold + 1} æŠ˜ AUC: {fold_auc:.4f}")
            
            fold_pbar.set_postfix_str(f"âœ… LGB: {lgb_auc:.3f}, XGB: {xgb_auc:.3f}, å¹³å‡: {fold_auc:.3f}")
        
        fold_pbar.close()
        return oof_predictions
    
    def fit(self, X: pd.DataFrame, y: pd.Series) -> 'StackingModel':
        """
        è¨“ç·´ Stacking æ¨¡å‹
        
        Args:
            X: ç‰¹å¾µæ•¸æ“š
            y: ç›®æ¨™è®Šæ•¸
            
        Returns:
            è¨“ç·´å¥½çš„æ¨¡å‹
        """
        logger.info("é–‹å§‹è¨“ç·´ Stacking æ¨¡å‹...")
        
        # å‰µå»ºç¸½é«”é€²åº¦æ¢
        total_steps = 4  # Base Models + Meta Model + Performance + SHAP
        main_pbar = tqdm(
            total=total_steps,
            desc="ğŸš€ Stacking æ¨¡å‹è¨“ç·´",
            unit="æ­¥é©Ÿ",
            ncols=100,
            file=sys.stdout
        )
        
        try:
            # æ­¥é©Ÿ 1: è¨“ç·´ Base Models
            main_pbar.set_description("ğŸ”„ è¨“ç·´ Base Models")
            oof_predictions = self._train_base_models(X, y)
            main_pbar.update(1)
            
            # æ­¥é©Ÿ 2: è¨“ç·´ Meta Model
            main_pbar.set_description("ğŸ§  è¨“ç·´ Meta Model")
            logger.info("è¨“ç·´ Meta Model (Logistic Regression)...")
            self.meta_model.fit(oof_predictions, y)
            main_pbar.update(1)
            
            # æ­¥é©Ÿ 3: è¨ˆç®—æ•´é«”æ€§èƒ½
            main_pbar.set_description("ğŸ“Š è¨ˆç®—æ¨¡å‹æ€§èƒ½")
            meta_pred = self.meta_model.predict_proba(oof_predictions)[:, 1]
            overall_auc = roc_auc_score(y, meta_pred)
            overall_acc = accuracy_score(y, (meta_pred >= 0.5).astype(int))
            
            # å­˜å„² CV åˆ†æ•¸
            self.cv_scores = {
                'mean_auc': float(overall_auc),
                'mean_accuracy': float(overall_acc),
                'cv_folds': self.cv_folds,
                'total_samples': len(X),
                'features_count': X.shape[1]
            }
            
            logger.info(f"Stacking æ¨¡å‹è¨“ç·´å®Œæˆ!")
            logger.info(f"æ•´é«” AUC: {overall_auc:.4f}")
            logger.info(f"æ•´é«”æº–ç¢ºç‡: {overall_acc:.4f}")
            main_pbar.update(1)
            
            # æ­¥é©Ÿ 4: è¨ˆç®— SHAP å€¼
            main_pbar.set_description("ğŸ” è¨ˆç®— SHAP å€¼")
            self._compute_shap_values(X, y)
            main_pbar.update(1)
            
            main_pbar.set_description("âœ… è¨“ç·´å®Œæˆ")
            main_pbar.close()
            
        except Exception as e:
            main_pbar.close()
            raise e
        
        self.is_fitted = True
        return self
    
    def predict_proba(self, X: pd.DataFrame) -> np.ndarray:
        """
        é æ¸¬æ©Ÿç‡
        
        Args:
            X: ç‰¹å¾µæ•¸æ“š
            
        Returns:
            é æ¸¬æ©Ÿç‡
        """
        if not self.is_fitted:
            raise ValueError("æ¨¡å‹å°šæœªè¨“ç·´ï¼Œè«‹å…ˆèª¿ç”¨ fit() æ–¹æ³•")
        
        # Base Models é æ¸¬
        base_predictions = self._get_base_predictions(X)
        
        # Meta Model é æ¸¬
        final_pred = self.meta_model.predict_proba(base_predictions)[:, 1]
        
        return final_pred
    
    def predict(self, X: pd.DataFrame, threshold: float = 0.5) -> np.ndarray:
        """
        é æ¸¬é¡åˆ¥
        
        Args:
            X: ç‰¹å¾µæ•¸æ“š
            threshold: åˆ†é¡é–¾å€¼
            
        Returns:
            é æ¸¬é¡åˆ¥
        """
        probas = self.predict_proba(X)
        return (probas >= threshold).astype(int)
    
    def _get_base_predictions(self, X: pd.DataFrame) -> np.ndarray:
        """
        ç²å¾— Base Models çš„å¹³å‡é æ¸¬
        
        Args:
            X: ç‰¹å¾µæ•¸æ“š
            
        Returns:
            Base Models é æ¸¬çµæœ
        """
        n_samples = len(X)
        base_predictions = np.zeros((n_samples, 2))
        
        # LightGBM é æ¸¬ (å¤šå€‹ fold çš„å¹³å‡)
        lgb_preds = []
        for lgb_model in self.base_models['lgb']:
            lgb_pred = lgb_model.predict(X, num_iteration=lgb_model.best_iteration)
            lgb_preds.append(lgb_pred)
        base_predictions[:, 0] = np.mean(lgb_preds, axis=0)
        
        # XGBoost é æ¸¬ (å¤šå€‹ fold çš„å¹³å‡)
        xgb_preds = []
        for xgb_model in self.base_models['xgb']:
            # ä½¿ç”¨æ¨™æº–çš„ XGBClassifier predict_proba
            xgb_pred = xgb_model.predict_proba(X)[:, 1]
            xgb_preds.append(xgb_pred)
        base_predictions[:, 1] = np.mean(xgb_preds, axis=0)
        
        return base_predictions
    
    def _compute_shap_values(self, X: pd.DataFrame, y: pd.Series):
        """
        è¨ˆç®— SHAP å€¼ç”¨æ–¼æ¨¡å‹è§£é‡‹
        
        Args:
            X: ç‰¹å¾µæ•¸æ“š
            y: ç›®æ¨™è®Šæ•¸
        """
        try:
            logger.info("è¨ˆç®— SHAP å€¼...")
            
            # å‰µå»º SHAP è¨ˆç®—é€²åº¦æ¢
            shap_pbar = tqdm(
                total=3,
                desc="ğŸ” SHAP è§£é‡‹è¨ˆç®—",
                unit="æ­¥é©Ÿ",
                ncols=100,
                file=sys.stdout
            )
            
            # æ­¥é©Ÿ 1: å»ºç«‹ SHAP è§£é‡‹å™¨
            shap_pbar.set_description("ğŸ”§ å»ºç«‹ SHAP è§£é‡‹å™¨")
            if self.base_models['xgb']:
                model_for_shap = self.base_models['xgb'][0]
                self.shap_explainer = shap.TreeExplainer(model_for_shap)
                shap_pbar.update(1)
                
                # æ­¥é©Ÿ 2: æº–å‚™æ¨£æœ¬æ•¸æ“š
                shap_pbar.set_description("ğŸ“Š æº–å‚™æ¨£æœ¬æ•¸æ“š")
                sample_size = min(1000, len(X))
                X_sample = X.sample(n=sample_size, random_state=self.random_state)
                shap_pbar.update(1)
                
                # æ­¥é©Ÿ 3: è¨ˆç®— SHAP å€¼
                shap_pbar.set_description("âš¡ è¨ˆç®— SHAP å€¼")
                shap_values = self.shap_explainer.shap_values(X_sample)
                
                # è¨ˆç®—å…¨åŸŸç‰¹å¾µé‡è¦æ€§ (å¹³å‡çµ•å° SHAP å€¼)
                mean_abs_shap = np.abs(shap_values).mean(0)
                feature_importance = list(zip(X.columns, mean_abs_shap))
                feature_importance.sort(key=lambda x: x[1], reverse=True)
                
                self.global_shap_values = feature_importance
                shap_pbar.update(1)
                
                shap_pbar.set_description("âœ… SHAP è¨ˆç®—å®Œæˆ")
                shap_pbar.close()
                logger.info("SHAP å€¼è¨ˆç®—å®Œæˆ")
            else:
                shap_pbar.close()
                logger.warning("æ²’æœ‰å¯ç”¨çš„ XGBoost æ¨¡å‹ä¾†è¨ˆç®— SHAP")
                
        except Exception as e:
            logger.warning(f"SHAP å€¼è¨ˆç®—å¤±æ•—: {str(e)}")
            self.global_shap_values = []
    
    def explain_prediction(self, X: pd.DataFrame) -> Tuple[np.ndarray, float]:
        """
        è§£é‡‹å–®ç­†é æ¸¬
        
        Args:
            X: å–®ç­†ç‰¹å¾µæ•¸æ“š
            
        Returns:
            SHAP å€¼å’ŒåŸºæº–å€¼
        """
        if self.shap_explainer is None:
            raise ValueError("SHAP è§£é‡‹å™¨æœªåˆå§‹åŒ–")
        
        shap_values = self.shap_explainer.shap_values(X)
        base_value = self.shap_explainer.expected_value
        
        return shap_values[0], base_value
    
    def get_feature_importance(self) -> List[Tuple[str, float]]:
        """
        ç²å¾—ç‰¹å¾µé‡è¦æ€§ (åŸºæ–¼ SHAP)
        
        Returns:
            ç‰¹å¾µé‡è¦æ€§åˆ—è¡¨
        """
        if self.global_shap_values is None:
            return []
        return self.global_shap_values
    
    def save_model(self, filepath: str):
        """
        ä¿å­˜æ¨¡å‹
        
        Args:
            filepath: ä¿å­˜è·¯å¾‘
        """
        model_data = {
            'base_models': self.base_models,
            'meta_model': self.meta_model,
            'cv_folds': self.cv_folds,
            'random_state': self.random_state,
            'lgb_params': self.lgb_params,
            'xgb_params': self.xgb_params,
            'is_fitted': self.is_fitted,
            'cv_scores': self.cv_scores,
            'global_shap_values': self.global_shap_values
        }
        
        with open(filepath, 'wb') as f:
            pickle.dump(model_data, f)
        
        logger.info(f"æ¨¡å‹å·²ä¿å­˜è‡³: {filepath}")
    
    @classmethod
    def load_model(cls, filepath: str) -> 'StackingModel':
        """
        è¼‰å…¥æ¨¡å‹
        
        Args:
            filepath: æ¨¡å‹è·¯å¾‘
            
        Returns:
            è¼‰å…¥çš„æ¨¡å‹
        """
        with open(filepath, 'rb') as f:
            model_data = pickle.load(f)
        
        # å‰µå»ºæ–°å¯¦ä¾‹
        model = cls(
            cv_folds=model_data['cv_folds'],
            random_state=model_data['random_state']
        )
        
        # æ¢å¾©æ¨¡å‹ç‹€æ…‹
        model.base_models = model_data['base_models']
        model.meta_model = model_data['meta_model']
        model.lgb_params = model_data['lgb_params']
        model.xgb_params = model_data['xgb_params']
        model.is_fitted = model_data['is_fitted']
        model.cv_scores = model_data.get('cv_scores', {})
        model.global_shap_values = model_data.get('global_shap_values', [])
        
        # é‡æ–°åˆå§‹åŒ– SHAP è§£é‡‹å™¨
        if model.base_models['xgb']:
            try:
                model.shap_explainer = shap.TreeExplainer(model.base_models['xgb'][0])
            except:
                logger.warning("ç„¡æ³•é‡æ–°åˆå§‹åŒ– SHAP è§£é‡‹å™¨")
        
        logger.info(f"æ¨¡å‹å·²å¾ {filepath} è¼‰å…¥")
        return model
    
    def get_cv_scores(self) -> Dict[str, float]:
        """
        ç²å–äº¤å‰é©—è­‰åˆ†æ•¸
        
        Returns:
            äº¤å‰é©—è­‰åˆ†æ•¸å­—å…¸
        """
        if not self.is_fitted:
            logger.warning("æ¨¡å‹å°šæœªè¨“ç·´ï¼Œç„¡æ³•ç²å– CV åˆ†æ•¸")
            return {}
        
        return self.cv_scores
